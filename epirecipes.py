# -*- coding: utf-8 -*-
"""EpiRecipes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZIacWyeFbLTSpJv1XRmfE2QOiU8ojTgB
"""

!pip install unidecode
!pip install pyspark

#Import the necessary libraries
import unidecode
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, explode, array, count, expr

"""UPLOAD DATASET FROM KAGGLE TO GOOGLE DRIVE"""

#Upload google drive to access the file
from google.colab import drive
drive.mount('/content/drive')

#Get the file path and upload it in a dataframe
file_path = "/content/drive/MyDrive/Dataset/epi_r.csv"

df = pd.read_csv(file_path)

#Print the head of the dataframe
df.head()

'''
Alternative method:
from google.colab import files
uploaded = files.upload()

# Load into a DataFrame
df = pd.read_csv("epi_r.csv")
df.head()'''

"""DATA CLEANING"""

#Doing preliminary analysis on the dataset
df.info()

df.describe()

df.dtypes

#Missing values analysis
missing_values = df.isnull().sum()
missing_percentage = (missing_values / len(df)) * 100

# Create a summary DataFrame for easier visualization
missing_summary = pd.DataFrame({
    'Missing Values': missing_values,
    'Percentage': missing_percentage
})

print(missing_summary)

#Based on the summary, drop the rows with NaN values
df = df.dropna()
#Check the updated version
df.info()

#Check for any duplicated rows
# Find duplicated rows
duplicated_rows = df[df.duplicated(keep=False)]

# Display duplicated rows
print("Duplicated Rows:")
print(duplicated_rows)

'''Based on the dataset, I will not remove the duplicate rows as the
title is unique and it will be helpful to identify the recipes'''

# Rename columns by removing the '#' character
df.rename(columns=lambda x: x.lstrip('#'), inplace=True)

# Normalize column names by removing accents and special characters
df.columns = df.columns.str.replace(' ', '_')  # Replace spaces with underscores
df.columns = [unidecode.unidecode(col) for col in df.columns]  # Remove accents
df.columns = df.columns.str.replace(r'[,._/&+\-\']', '', regex=True)

#Removing unnecessary columns
'''Since this dataset contains 680 columns, I will reduce it by
removing the columns where all the row values are equal to 0 '''

df = df.loc[:, (df != 0).any(axis=0)]
print("Remaining columns:\n", df.columns)

#Detecting outliers in the rating column and removing them using boxplot and IQR
sns.boxplot(x=df['rating'])
plt.title("Boxplot of rating")
plt.show()

q1 = df['rating'].quantile(0.25)
q3 = df['rating'].quantile(0.75)
iqr = q3 - q1
lower_bound = q1 - 1.5 * iqr
upper_bound = q3 + 1.5 * iqr

# Keep only values within IQR bounds
df = df[(df['rating'] >= lower_bound) & (df['rating'] <= upper_bound)]

df.shape
df.info()

"""EXPLORATORY DATA ANALYSIS"""

#Histogram of recipe ratings: Distribution of recipes
sns.histplot(df['rating'], bins=20, kde=True)
plt.title("Histogram of recipe ratings")
plt.xlabel("Rating")
plt.ylabel("Frequency")
plt.grid(axis='y')
plt.show()

'''Since the data set is large, I am going to use PySpark for analysis along
  with Python'''
spark = SparkSession.builder \
    .master("local") \
    .appName("RecipeDataAnalysis") \
    .getOrCreate()

#Creating a seperate dataframe to use in spark
spark_df = spark.createDataFrame(df)

#Extract the top 5 foods with the highest calories
highest_calorie_df = spark_df.orderBy(spark_df["calories"].desc()).select("title", "calories").limit(5)

#Convert the Spark DataFrame to Pandas DataFrame for visualization
highest_calorie_pd = highest_calorie_df.toPandas()

#Create a bar plot using Seaborn
sns.barplot(x="title", y="calories", data=highest_calorie_pd)
plt.title("Top 5 Foods with the Highest Calories")
plt.xlabel("Food Name")
plt.ylabel("Calories")
plt.xticks(rotation=45, ha='right')
plt.show()

#Scatter plot to show the relationship between calories and fat
calorie_fat_df = spark_df.select("calories", "fat").toPandas()

# Scatter Plot for Calories vs. Fat
plt.scatter(calorie_fat_df["calories"], calorie_fat_df["fat"], color='blue')
plt.title("Scatter Plot: Calories vs Fat")
plt.xlabel("Calories")
plt.ylabel("Fat")
plt.show()

#Scatter plot to show the relationship between protein and fat
calorie_fat_df = spark_df.select("protein", "fat").toPandas()

# Scatter Plot for Calories vs. Fat
plt.scatter(calorie_fat_df["protein"], calorie_fat_df["fat"], color='red')
plt.title("Scatter Plot: Protien vs Fat")
plt.xlabel("Protein")
plt.ylabel("Fat")
plt.show()

'''Using heatmap to visualize the correlation between calories, protein, fat, and sodium.'''

correlation_matrix = df[['calories', 'protein', 'fat', 'sodium']].corr()
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='Spectral', fmt=".2f")
plt.title("Correlation Heatmap of Nutritional Components")
plt.show()

'''Creating a stacked bar plot To show the proportion of different recipe types
  (e.g., vegan, vegetarian) across various calorie ranges.'''
bins = [0, 200, 400, 600, 800, 1000]
labels = ['0-200', '201-400', '401-600', '601-800', '801-1000']
df['calorie_bins'] = pd.cut(df['calories'], bins=bins, labels=labels)

# Count the occurrences of each vegan/non-vegan in each calorie bin
diet_counts = df.groupby(['calorie_bins', 'vegan']).size().unstack(fill_value=0)

diet_counts.plot(kind='bar', stacked=True, color=['red', 'green'])
plt.title("Dietary Restrictions Across Calorie Ranges")
plt.xlabel("Calorie Ranges")
plt.ylabel("Number of Recipes")
plt.legend(title='Diet Type', labels=['Non-Vegan', 'Vegan'])
plt.show()


'''Insight gained: Vegan recipes have lesser calories as compared
  to non-vegan recipes. '''

'''Plotting a graph to depict the count of recipes with relation to dietary restrictions '''
# Create a list of dietary preference columns
dietary_columns = ["vegan", "vegetarian", "wheatglutenfree", "lowcarb", "dairyfree", "lowsugar", "peanutfree"]

# Transform the DataFrame to long format using selectExpr
melted_df = spark_df.selectExpr("stack({0}, {1}) as (dietary_preference, is_preferred)".format(
    len(dietary_columns),
    ', '.join(["'{0}', `{0}`".format(col_name) for col_name in dietary_columns])
))

# Filter to keep only the preferred dietary preferences
count_df = melted_df.filter(col("is_preferred") == '1') \
    .groupBy("dietary_preference") \
    .agg(count("*").alias("count"))

# Collect the results into a Pandas DataFrame
pandas_df = count_df.toPandas()

# Plotting
plt.figure(figsize=(10, 6))
plt.bar(pandas_df['dietary_preference'], pandas_df['count'], color='purple')
plt.title('Dietary Preferences Count')
plt.xlabel('Dietary Preference')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.grid(axis='y')

# Show the plot
plt.tight_layout()
plt.show()

''' Insight gain: Peanut-free recipes are gaining popularity as compared
  to other dietary restrictions. Low-sugar recipes are the least'''

'''Plotting a pie chart to depict the relationship between seasons and number of recipes '''
from pyspark.sql.functions import col, sum as spark_sum
# Define season columns
season_columns = ["summer", "winter", "spring", "fall"]  # Adjust the list based on your DataFrame

# Create a new DataFrame to sum up the number of recipes in each season
season_counts = spark_df.select([spark_sum(col(season)).alias(season) for season in season_columns]).collect()[0]

# Prepare data for plotting
season_labels = season_columns
season_sizes = [season_counts[season] for season in season_columns]

# Create a pie chart
plt.figure(figsize=(8, 8))
plt.pie(season_sizes,
        labels=season_labels,
        autopct='%1.1f%%',
        startangle=140,
        colors=['#ff9999', '#66b3ff', '#99ff99', '#ffcc99'],  # Custom colors
        explode=[0.1] * len(season_sizes))

plt.title('Distribution of Recipes by Season', fontsize=16, fontweight='bold')
plt.axis('equal')

plt.show()

'''Insight gained: The majority of recipes are made for summer, with
  fall and winter coming a close second. The least recipes are in spring '''